{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Statistical Analysis of Empiricist Experiment Data",
   "id": "2e7af3eb22c36387"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T01:58:23.193245Z",
     "start_time": "2025-11-29T01:58:23.183201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import statistics"
   ],
   "id": "1e0b5cbf8bfe955a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Parsing the original analysis_results.jsonl to extract o1 high and o1 low results into separate files, assuming that these results are from evaluating with Sonnet 3.5 as the judge model.",
   "id": "41ea6fda4b35ce2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T02:14:11.252405Z",
     "start_time": "2025-11-29T02:14:11.212760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "official_results = '../analysis_results.jsonl'\n",
    "\n",
    "# parse for o1 high results\n",
    "o1_high_results = []\n",
    "with open(official_results, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if data['model'] == 'o1_maxiter_30_N_v0.20.0-no-hint-run_1':\n",
    "            o1_high_results.append(data)\n",
    "\n",
    "# save to file for later\n",
    "with open('official/o1_high_results.jsonl', 'w') as f:\n",
    "    for result in o1_high_results:\n",
    "        f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "o1_low_results = []\n",
    "with open(official_results, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if data['model'] == 'o1_low_maxiter_30_N_v0.20.0-no-hint-run_1':\n",
    "            o1_low_results.append(data)\n",
    "\n",
    "# save to file for later\n",
    "with open('official/o1_low_results.jsonl', 'w') as f:\n",
    "    for result in o1_low_results:\n",
    "        f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "len(o1_high_results), len(o1_low_results)"
   ],
   "id": "ac1d7b0d7b36785d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 199)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quick function to calculate mean and standard deviation of overthinking scores from a given file.",
   "id": "99cd43aa34aeb7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T03:20:23.518765Z",
     "start_time": "2025-11-29T03:20:23.511054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_stats(filepath, sample_size=None):\n",
    "    scores = []\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            scores.append(float(data['overthinking_score']))\n",
    "\n",
    "    if sample_size is not None:\n",
    "        scores = scores[:sample_size]\n",
    "\n",
    "    avg = statistics.mean(scores)\n",
    "    std_dev = statistics.stdev(scores)\n",
    "\n",
    "    return avg, std_dev, len(scores)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, I analyze both official and my_eval files for o1 high and o1 low results to verify the author's claim that overthinking is more prevalent in o1_low model than in o1_high model. I use a sample size of 50 due to limited data in my_eval files.\n",
    "\n",
    "my_eval files were evaluated using Anthropic's Haiku 3.5 model instead of Sonnet 3.5. I assumed that the Haiku model would perform similarly to Sonnet as an LLM-as-a-judge. It was used to cut costs and speed up the evaluation process."
   ],
   "id": "1eaef31f7746db8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:21:40.556104Z",
     "start_time": "2025-11-29T05:21:40.542341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analyze files\n",
    "files = [\n",
    "    'official/o1_high_results.jsonl',\n",
    "    'official/o1_low_results.jsonl',\n",
    "    'my_eval/o1_high_overthinking.jsonl',\n",
    "    'my_eval/o1_low_overthinking.jsonl'\n",
    "]\n",
    "\n",
    "for filepath in files:\n",
    "    avg, std_dev, count = calc_stats(filepath, sample_size=50)\n",
    "    print(f\"{filepath}: {avg:.02f} ± {std_dev:.02f}\")"
   ],
   "id": "6113508b78729458",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "official/o1_high_results.jsonl: 1.06 ± 0.89\n",
      "official/o1_low_results.jsonl: 3.46 ± 3.57\n",
      "my_eval/o1_high_overthinking.jsonl: 2.98 ± 2.51\n",
      "my_eval/o1_low_overthinking.jsonl: 3.00 ± 2.16\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
