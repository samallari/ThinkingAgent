This directory contains the code I wrote for an experiment I conducted as part of my presentation on [The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks](https://arxiv.org/pdf/2502.08235) for ECE 605: Large-Scale AI (Fall 2025) seminar.

The experiment focuses on investigating the paper's claim regarding overthinking in large language models (LLMs) when used as agents and its correlation with token usage. The authors observed that OpenAI's o1 model overthinks MORE when it is configured to use LOW reasoning effort, and overthinks LESS with HIGH reasoning effort. This claim directly challenges previous claims that higher token usage leads to more overthinking, thus hurting the performance of the model. The researchers suggest that structured, extended reasoning processes help the model work through problems more effectively, rather than causing it to spiral into unnecessary complexity.