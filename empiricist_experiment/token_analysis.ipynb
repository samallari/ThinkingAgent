{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7af3eb22c36387",
   "metadata": {},
   "source": [
    "# Statistical Analysis of Empiricist Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e0b5cbf8bfe955a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T01:58:23.193245Z",
     "start_time": "2025-11-29T01:58:23.183201Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea6fda4b35ce2e",
   "metadata": {},
   "source": [
    "Parsing the original analysis_results.jsonl to extract o1 high and o1 low results into separate files, assuming that these results are from evaluating with Sonnet 3.5 as the judge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac1d7b0d7b36785d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T02:14:11.252405Z",
     "start_time": "2025-11-29T02:14:11.212760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 199)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "official_results = '../analysis_results.jsonl'\n",
    "\n",
    "# parse for o1 high results\n",
    "o1_high_results = []\n",
    "with open(official_results, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if data['model'] == 'o1_maxiter_30_N_v0.20.0-no-hint-run_1':\n",
    "            o1_high_results.append(data)\n",
    "\n",
    "# save to file for later\n",
    "with open('official/o1_high_results.jsonl', 'w') as f:\n",
    "    for result in o1_high_results:\n",
    "        f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "o1_low_results = []\n",
    "with open(official_results, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if data['model'] == 'o1_low_maxiter_30_N_v0.20.0-no-hint-run_1':\n",
    "            o1_low_results.append(data)\n",
    "\n",
    "# save to file for later\n",
    "with open('official/o1_low_results.jsonl', 'w') as f:\n",
    "    for result in o1_low_results:\n",
    "        f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "len(o1_high_results), len(o1_low_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd43aa34aeb7e",
   "metadata": {},
   "source": [
    "Quick function to calculate mean and standard deviation of overthinking scores from a given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T03:20:23.518765Z",
     "start_time": "2025-11-29T03:20:23.511054Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_stats(filepath, sample_size=None):\n",
    "    scores = []\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            scores.append(float(data['overthinking_score']))\n",
    "\n",
    "    if sample_size is not None:\n",
    "        scores = scores[:sample_size]\n",
    "\n",
    "    avg = statistics.mean(scores)\n",
    "    std_dev = statistics.stdev(scores)\n",
    "\n",
    "    return avg, std_dev, len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaef31f7746db8c",
   "metadata": {},
   "source": [
    "Next, I analyze both official and my_eval files for o1 high and o1 low results to verify the author's claim that overthinking is more prevalent in o1_low model than in o1_high model. I use a sample size of 50 due to limited data in my_eval files.\n",
    "\n",
    "my_eval files were evaluated using Anthropic's Haiku 3.5 model instead of Sonnet 3.5. I assumed that the Haiku model would perform similarly to Sonnet as an LLM-as-a-judge. It was used to cut costs and speed up the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6113508b78729458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T05:21:40.556104Z",
     "start_time": "2025-11-29T05:21:40.542341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "official/o1_high_results.jsonl: 1.06 ± 0.89\n",
      "official/o1_low_results.jsonl: 3.46 ± 3.57\n",
      "my_eval/o1_high_overthinking.jsonl: 2.98 ± 2.51\n",
      "my_eval/o1_low_overthinking.jsonl: 3.00 ± 2.16\n"
     ]
    }
   ],
   "source": [
    "# Analyze files\n",
    "files = [\n",
    "    'official/o1_high_results.jsonl',\n",
    "    'official/o1_low_results.jsonl',\n",
    "    'my_eval/o1_high_overthinking.jsonl',\n",
    "    'my_eval/o1_low_overthinking.jsonl'\n",
    "]\n",
    "\n",
    "for filepath in files:\n",
    "    avg, std_dev, count = calc_stats(filepath, sample_size=50)\n",
    "    print(f\"{filepath}: {avg:.02f} ± {std_dev:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e802e",
   "metadata": {},
   "source": [
    "The next steps are to evaluate overthinking scores from a different model. There were several ways I thought about to force \"low reasoning effort\" and \"high reasoning effort\". At first, I attempted to use Deepseek-R1-Distill-Qwen-14B, since the model's trajectories are already provided in the repository. I considered these trajectories as \"high reasoning effort\" as I assumed that it had an unlimited token budget. My thinking was that I could run Deepseek-R1-Distill-Qwen-14B with max_token set for my \"low reasoning effort\" trajectories.\n",
    "\n",
    "Since I was going to use the OpenHands framework to evaluate the model using SWE-Bench, I had to set this up first. However this is where I hit my first major roadblock. Since OpenHands relies on docker, I couldn't run OpenHands on KOA (HPC server), meaning I needed to host the model remotely and then run OpenHands eval locally on my computer. Figuring out how to host the model remotely on the server and expose it publicly to my computer was a bit too difficult for me, so I looked into alternative models with an API that I could run the benchmark on instead. This led to my choice in evaluating Gemini-2.5-Flash. The Gemini API allows you to set \"low reasoning effort\" and \"high reasoning effort\", which was perfect for my experiment! \n",
    "\n",
    "Now that I had the model setup, I just needed to run the models on SWE-Bench. This is where I hit my second major roadblock. I needed to setup OpenHands locally on my computer, however my docker builds kept crashing on my Mac... I suspected it was due to the docker builds were built on x86_64 and my Mac uses an M2 chip. Luckily, I have a Windows PC with WSL setup at my parents' house, so I planned to use it over Thanksgiving. Due to limitations on time, RAM, and storage, the evaluations I ran on Gemini are very limited.\n",
    "\n",
    "I ran the evaluations with the following commands on OpenHands:\n",
    "```bash\n",
    "./evaluation/benchmarks/swe_bench/scripts/run_infer.sh llm.eval_gemini_high HEAD CodeActAgent 10 10 1 princeton-nlp/SWE-bench_Verified test\n",
    "\n",
    "./evaluation/benchmarks/swe_bench/scripts/run_infer.sh llm.eval_gemini_low HEAD CodeActAgent 10 10 1 princeton-nlp/SWE-bench_Verified test\n",
    "```\n",
    "Where `eval_limit=10` `max_iter=10` `num_workers=1`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
